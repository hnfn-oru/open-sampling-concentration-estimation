# -*- coding: utf-8 -*-
"""
Created on Thu Apr 10 11:20:30 2025

Author: fanha

This script demonstrates the following workflow:
  1. Data loading and preprocessing from sensor data.
  2. Defining an LSTM model training function that accepts hyperparameters including:
       - Number of LSTM layers,
       - Number of LSTM units,
       - Learning rate,
       - Batch size.
  3. Using Bayesian Optimization to tune these hyperparameters.
  4. Building and training a final LSTM model using the best hyperparameters.
  5. Evaluating and visualizing the model predictions against actual target values.
"""

import numpy as np
import matplotlib.pyplot as plt

# Import components from Keras to build and train the model.
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input  # Note: Input is used to explicitly define the input shape.
from tensorflow.keras.optimizers import Adam

# Import MSE metric from scikit-learn to evaluate model performance.
from sklearn.metrics import mean_squared_error

# Import BayesianOptimization to perform hyperparameter tuning.
from bayes_opt import BayesianOptimization

# Import a custom data processing class that handles sensor data.
from LST_SensorDataProcessor import SensorDataProcessor

###############################################################
# Step 1: Data Loading and Preprocessing
###############################################################
# Initialize the custom SensorDataProcessor with a specified base path where the data is stored.
processor = SensorDataProcessor(base_path="train/experiment")

# Obtain the regression data from a particular sensor type. The function
# prepare_regression_data is custom implemented based on specific user needs.
data4reg = processor.prepare_regression_data(sensor_type='MiCS5524', num_layers=4)

# Obtain the target data from another sensor type (or source) for regression purposes.
data4target = processor.prepare_target_data(sensor_type='PID-sensor', num_layers=4)

# Generate training and test datasets from the sensor data.
# The function prepare_training_data splits the data with a given train_portion (80% for training),
# uses a sequence length (seq_len) of 80 time steps, and selects a specific layer (target_layer=0)
# for the target variable. Initially, the input X has shape (number_of_samples, 80)
# and the targets y is a 1D array with shape (number_of_samples,).
X_train, X_test, y_train, y_test = processor.prepare_training_data(
    data4reg, data4target, train_portion=0.8, seq_len=80, target_layer=0
)

# Print out the shapes of the training and testing data to verify they are correctly formed.
print("Training data shape:", X_train.shape, y_train.shape)
print("Testing data shape:", X_test.shape, y_test.shape)

# LSTM models require the input data to be a 3D tensor of shape:
#     (samples, time_steps, features)
# Since X_train and X_test are currently 2D (samples, 80), we add an extra dimension to indicate one feature per time step.
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

###############################################################
# Step 2: Define the LSTM Training Function for Bayesian Optimization
###############################################################
def train_lstm_model(num_layers, units, learning_rate, batch_size):
    """
    Build and train an LSTM model with a variable number of layers.
    Returns the negative Mean Squared Error (MSE) on the test set for use in Bayesian Optimization.
    (We return negative MSE because BayesianOptimization maximizes the objective function.)

    Parameters:
      - num_layers (float): Number of LSTM layers to stack (will be converted to int).
      - units (float): Number of units in each LSTM layer (converted to int).
      - learning_rate (float): Learning rate for the Adam optimizer.
      - batch_size (float): Batch size for training (converted to int).

    Returns:
      - Negative validation loss (MSE) evaluated on the test set.
    """
    # Convert hyperparameters that are expected as integers.
    num_layers = int(num_layers)
    units = int(units)
    batch_size = int(batch_size)
    
    # Build the model using the Sequential API.
    model = Sequential()
    # Use an explicit Input layer to specify the input shape without passing it directly to LSTM.
    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))
    
    # Dynamically add LSTM layers based on the num_layers parameter.
    if num_layers == 1:
        # For a single layer, simply add one LSTM layer without returning sequences.
        model.add(LSTM(units, return_sequences=False))
    else:
        # For more than one layer, add the first LSTM layer with return_sequences=True.
        model.add(LSTM(units, return_sequences=True))
        # Add intermediate LSTM layers if there are more than 2 layers.
        for _ in range(num_layers - 2):
            model.add(LSTM(units, return_sequences=True))
        # Add the final LSTM layer with return_sequences=False.
        model.add(LSTM(units, return_sequences=False))
    
    # Add the final Dense layer with one neuron (for regression output).
    model.add(Dense(1))
    
    # Compile the model using Adam optimizer and mean squared error loss.
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    
    # Train the model for 10 epochs using the provided batch size. The training process is silent (verbose=0).
    model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=0)
    
    # Evaluate the model performance on the test data.
    val_loss = model.evaluate(X_test, y_test, verbose=0)
    # Return negative MSE so that the Bayesian optimizer can maximize this value.
    return -val_loss

###############################################################
# Step 3: Hyperparameter Tuning via Bayesian Optimization
###############################################################
# Define the parameter search space for Bayesian Optimization.
pbounds = {
    'num_layers': (1, 3),          # Search for between 1 and 3 LSTM layers.
    'units': (10, 100),            # Number of LSTM units between 10 and 100.
    'learning_rate': (1e-4, 1e-2),   # Learning rate range from 0.0001 to 0.01.
    'batch_size': (16, 128)        # Batch size between 16 and 128.
}

# Initialize the Bayesian Optimizer with the training function and specified parameter bounds.
optimizer = BayesianOptimization(
    f=train_lstm_model,
    pbounds=pbounds,
    random_state=42  # Set random_state for reproducibility.
)

# Run the optimization: 5 initial random points and 10 iterations of optimization.
optimizer.maximize(init_points=5, n_iter=10)

# Print out the best hyperparameters and associated result found during optimization.
print("Best parameters from Bayesian Optimization:")
print(optimizer.max)

# Extract the best parameters for use in the final model.
best_params = optimizer.max['params']
best_units = int(best_params['units'])
best_learning_rate = best_params['learning_rate']
best_batch_size = int(best_params['batch_size'])
# Note: best_params['num_layers'] is available if needed by the final model building function.

###############################################################
# Step 4: Build and Train the Final Model (Using the Best Hyperparameters)
###############################################################
def build_lstm_model(input_shape, num_layers, units, learning_rate):
    """
    Builds and compiles an LSTM model with a variable number of stacked layers.
    Uses an explicit Input layer to define input shape and dynamically stacks LSTM layers based on the parameter.
    
    Parameters:
      - input_shape (tuple): Shape of input data (time_steps, features), e.g., (80, 1).
      - num_layers (int): Number of LSTM layers to be stacked.
      - units (int): Number of units in each LSTM layer.
      - learning_rate (float): Learning rate for the optimizer.
      
    Returns:
      - model: A compiled Keras Sequential model.
    """
    model = Sequential()
    # Define the input for the model explicitly.
    model.add(Input(shape=input_shape))
    
    # Dynamically add LSTM layers.
    if num_layers == 1:
        # Add a single LSTM layer.
        model.add(LSTM(units, return_sequences=False))
    else:
        # Add the first LSTM layer with return_sequences enabled.
        model.add(LSTM(units, return_sequences=True))
        # Add any intermediate layers (if any).
        for _ in range(num_layers - 2):
            model.add(LSTM(units, return_sequences=True))
        # Add the final LSTM layer.
        model.add(LSTM(units, return_sequences=False))
    
    # Add a Dense output layer for regression.
    model.add(Dense(1))
    
    # Compile the model with the Adam optimizer and Mean Squared Error loss.
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')
    return model

# Note: When building the final model, you could use the best found 'num_layers' too.
# Here, for demonstration, we assume best_params contains the correct 'num_layers'.
# In case you want to use it, you would do:
final_model = build_lstm_model(
    (X_train.shape[1], X_train.shape[2]),
    num_layers=int(optimizer.max['params']['num_layers']),
    units=best_units,
    learning_rate=best_learning_rate
)

# Train the final model for a longer duration (20 epochs) using the best batch size.
final_model.fit(X_train, y_train, epochs=20, batch_size=best_batch_size, verbose=1)

###############################################################
# Step 5: Model Evaluation and Result Visualization
###############################################################
# Generate predictions from the final model using the test set.
predictions = final_model.predict(X_test)

# Calculate the Mean Squared Error (MSE) and then compute the Root Mean Squared Error (RMSE) for interpretability.
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
print(f"Test RMSE: {rmse}")

# Plot the actual vs. predicted values to visually compare model performance.
plt.figure(figsize=(12, 6))
plt.plot(y_test, label="Actual Values")
plt.plot(predictions, label="Predicted Values", linestyle="--")
plt.title("Actual vs Predicted Values")
plt.xlabel("Sample Index")
plt.ylabel("Value")
plt.legend()
plt.grid()
plt.show()
