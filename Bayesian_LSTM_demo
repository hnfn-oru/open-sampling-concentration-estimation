# -*- coding: utf-8 -*-
"""
Created on Thu Apr 10 11:20:30 2025

Author: fanha

This script demonstrates how to:
    1. Load and preprocess sensor data using a custom data processor.
    2. Define an LSTM model training function that will be optimized using Bayesian Optimization.
    3. Tune key hyperparameters (LSTM units, learning rate, and batch size) with Bayesian optimization.
    4. Build and train the final LSTM model using the best hyperparameters.
    5. Evaluate the final model's performance and visualize the predictions.
"""

import numpy as np
import matplotlib.pyplot as plt

# Import necessary components from Keras for model building, training, and optimization.
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam

# Import mean squared error (MSE) calculation from scikit-learn for performance evaluation.
from sklearn.metrics import mean_squared_error

# Import the BayesianOptimization class for hyperparameter tuning.
from bayes_opt import BayesianOptimization

# Import the custom sensor data processing class.
from LST_SensorDataProcessor import SensorDataProcessor

###############################################################
# Step 1: Data Loading and Preprocessing
###############################################################
# Initialize the SensorDataProcessor with the base directory path containing the data.
processor = SensorDataProcessor(base_path="train/experiment")

# Retrieve the regression data from a sensor; the implementation of this function
# should load and possibly preprocess raw sensor data for a regression task.
data4reg = processor.prepare_regression_data(sensor_type='MiCS5524', num_layers=4)

# Retrieve the target data for the sensor. This might be data from another sensor or
# the ground truth values that are used as targets in the regression.
data4target = processor.prepare_target_data(sensor_type='PID-sensor', num_layers=4)

# Use the custom processor to prepare the training and testing datasets.
# The following function splits the data based on a specified training portion, 
# sets the sequence length (seq_len) for time series sequences (80 time steps in this case),
# and selects the target layer (layer index 0 in this example).
X_train, X_test, y_train, y_test = processor.prepare_training_data(
    data4reg, data4target, train_portion=0.8, seq_len=80, target_layer=0
)

# Print out the shapes of the generated training and testing sets to verify dimensions.
# Expected shapes: Training: (num_train_samples, 80) for X_train and (num_train_samples,) for y_train;
#                  Testing:  (num_test_samples, 80) for X_test and (num_test_samples,) for y_test.
print("Training data shape:", X_train.shape, y_train.shape)
print("Testing data shape:", X_test.shape, y_test.shape)

# The LSTM model in Keras expects input data as a three-dimensional tensor with shape:
#   (number_of_samples, time_steps, number_of_features)
# Since X_train and X_test are currently 2D arrays with shape (samples, time_steps),
# we need to add a third dimension, representing a single feature channel.
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

###############################################################
# Step 2: Define the LSTM Model Training Function for Bayesian Optimization
###############################################################
def train_lstm_model(units, learning_rate, batch_size):
    """
    Build and train an LSTM model using specified hyperparameters, and return the negative
    Mean Squared Error (MSE) on the testing set. The negative MSE is returned since the 
    Bayesian optimization routine maximizes the objective, so minimizing MSE is achieved by
    maximizing its negative value.
    
    Parameters:
      - units (float): Number of units in the LSTM layer. This value is converted to int.
      - learning_rate (float): Learning rate for the Adam optimizer.
      - batch_size (float): Batch size for model training. Also converted to int.
      
    Returns:
      - Negative validation loss (MSE) measured on the test set.
    """
    # Convert hyperparameters that must be integers to int type.
    units = int(units)
    batch_size = int(batch_size)
    
    # Create the LSTM model using the Sequential API.
    # Input shape is set to (time_steps, features), where time_steps = X_train.shape[1] (should be 80)
    # and features = X_train.shape[2] (which is 1 after reshaping).
    model = Sequential([
        LSTM(units, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False),
        Dense(1)  # Single neuron output for a regression task.
    ])
    
    # Compile the model using the Adam optimizer and Mean Squared Error loss.
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    
    # Train the model on the training data for a fixed number of epochs (here 10).
    # Note: The batch_size is one of the variables to be optimized.
    model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=0)
    
    # Evaluate the model on the testing data to obtain validation loss.
    val_loss = model.evaluate(X_test, y_test, verbose=0)
    
    # Return the negative validation loss. Bayesian Optimization will try to maximize this value.
    return -val_loss

###############################################################
# Step 3: Hyperparameter Tuning Using Bayesian Optimization
###############################################################
# Define the search space for hyperparameters.
# 'units' refers to the number of LSTM units,
# 'learning_rate' is the step size for the optimizer,
# and 'batch_size' determines how many samples are processed before updating the model weights.
pbounds = {
    'units': (10, 100),           # LSTM units range from 10 to 100.
    'learning_rate': (1e-4, 1e-2),  # Learning rate between 0.0001 and 0.01.
    'batch_size': (16, 128)         # Batch size between 16 and 128.
}

# Initialize the Bayesian Optimization process with the training function and search bounds.
optimizer = BayesianOptimization(
    f=train_lstm_model,
    pbounds=pbounds,
    random_state=42  # Set random_state for reproducibility.
)

# Perform the Bayesian Optimization:
# 'init_points' specifies the number of random samples to explore at first,
# 'n_iter' determines the number of optimization iterations.
optimizer.maximize(init_points=5, n_iter=10)

# Print out the best-found parameters and the associated optimization result.
print("Best parameters from Bayesian Optimization:")
print(optimizer.max)

# Retrieve and convert the best hyperparameters from the optimization result.
best_params = optimizer.max['params']
best_units = int(best_params['units'])
best_learning_rate = best_params['learning_rate']
best_batch_size = int(best_params['batch_size'])

###############################################################
# Step 4: Build and Train the Final Model Using the Best Hyperparameters
###############################################################
def build_final_model(input_shape, units, learning_rate):
    """
    Create and compile the final LSTM model using the best hyperparameters.
    
    Parameters:
      - input_shape (tuple): Shape of the input data (time_steps, features).
      - units (int): The number of LSTM units.
      - learning_rate (float): Learning rate for the optimizer.
      
    Returns:
      - model: A compiled Keras Sequential model ready for training.
    """
    model = Sequential([
        LSTM(units, input_shape=input_shape, return_sequences=False),
        Dense(1)  # Output layer for regression.
    ])
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')
    return model

# Build the final model using the shape of the training data and the best hyperparameters.
final_model = build_final_model((X_train.shape[1], X_train.shape[2]), best_units, best_learning_rate)

# Train the final model for a longer duration (20 epochs in this example) using the best batch size.
final_model.fit(X_train, y_train, epochs=20, batch_size=best_batch_size, verbose=1)

###############################################################
# Step 5: Model Evaluation and Results Visualization
###############################################################
# Use the final model to generate predictions on the test data.
predictions = final_model.predict(X_test)

# Compute the Mean Squared Error (MSE) between predictions and actual test values.
mse = mean_squared_error(y_test, predictions)

# Calculate the Root Mean Squared Error (RMSE) for interpretability.
rmse = np.sqrt(mse)
print(f"Test RMSE: {rmse}")

# Plot the actual vs. predicted values for visual comparison.
plt.figure(figsize=(12, 6))
plt.plot(y_test, label="Actual Values")
plt.plot(predictions, label="Predicted Values", linestyle="--")
plt.title("Actual vs Predicted Values")
plt.xlabel("Sample Index")
plt.ylabel("Value")
plt.legend()
plt.show()
