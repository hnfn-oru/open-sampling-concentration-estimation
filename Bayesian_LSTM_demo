# -*- coding: utf-8 -*-
"""
Created on Thu Apr 10 11:20:30 2025

@author: fanha
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error
from bayes_opt import BayesianOptimization

# 导入自定义数据处理类
from LST_SensorDataProcessor import SensorDataProcessor

###############################################################
# Step 1: 数据加载与预处理
###############################################################
# 初始化处理器，指定数据基础路径
processor = SensorDataProcessor(base_path="train/experiment")

# 获取传感器回归数据和目标数据（这两个函数依据您的实际需求进行实现）
data4reg = processor.prepare_regression_data(sensor_type='MiCS5524', num_layers=4)
data4target = processor.prepare_target_data(sensor_type='PID-sensor', num_layers=4)

# 根据传感器数据生成训练和测试数据集，并设置序列长度（seq_len）为80
# 其中 X 数据的初始形状为 (样本数, 80) ，y 数据为一维数组 (样本数,)
X_train, X_test, y_train, y_test = processor.prepare_training_data(
    data4reg, data4target, train_portion=0.8, seq_len=80, target_layer=0
)

# 打印原始数据形状
print("Training data shape:", X_train.shape, y_train.shape)
print("Testing data shape:", X_test.shape, y_test.shape)

# LSTM 模型要求输入数据为三维张量：(样本数, 时间步, 特征数)
# 当前 X_train 和 X_test 为二维，需增加特征维度（此处特征数为1）
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

###############################################################
# Step 2: 定义使用贝叶斯优化的 LSTM 训练函数
###############################################################
from tensorflow.keras.layers import Input  # Ensure Input is imported

def train_lstm_model(num_layers, units, learning_rate, batch_size):
    """
    Build and train an LSTM model with a variable number of layers.
    The function returns the negative Mean Squared Error (MSE) on the test set
    for use in Bayesian Optimization. Using the negative MSE allows us to 
    maximize performance during optimization.

    Parameters:
      - num_layers (float): Number of LSTM layers to stack. This value will be converted to int.
      - units (float): Number of units in each LSTM layer (converted to int).
      - learning_rate (float): Learning rate for the Adam optimizer.
      - batch_size (float): Batch size for training (converted to int).

    Returns:
      - Negative validation loss (MSE) measured on the test set.
    """
    # Convert hyperparameters that require integer values.
    num_layers = int(num_layers)
    units = int(units)
    batch_size = int(batch_size)
    
    # Build the Sequential model with an explicit input layer.
    # X_train is assumed to be available in global scope with shape (samples, time_steps, features)
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))

    # Dynamically add LSTM layers based on the num_layers parameter.
    # For multiple layers, intermediate layers need return_sequences=True.
    if num_layers == 1:
        # Only one LSTM layer: no need to return sequences.
        model.add(LSTM(units, return_sequences=False))
    else:
        # Add the first LSTM layer with full sequence output.
        model.add(LSTM(units, return_sequences=True))
        # If more than two layers, add intermediate layers.
        for _ in range(num_layers - 2):
            model.add(LSTM(units, return_sequences=True))
        # Add the final LSTM layer without returning sequences.
        model.add(LSTM(units, return_sequences=False))
    
    # Add a Dense output layer with a single neuron for regression.
    model.add(Dense(1))
    
    # Compile the model with the Adam optimizer and Mean Squared Error as the loss function.
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    
    # Train the model using the training data.
    # Here we use 10 epochs for training during optimization.
    model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=0)
    
    # Evaluate the model on the test set and return negative loss for maximization.
    val_loss = model.evaluate(X_test, y_test, verbose=0)
    return -val_loss



###############################################################
# Step 3: 贝叶斯优化超参数调节
###############################################################
# 定义超参数搜索范围
pbounds = {
    'num_layers': (1, 3),          # For example, search for 1 to 3 LSTM layers
    'units': (10, 100),            # Number of LSTM units
    'learning_rate': (1e-4, 1e-2),   # Learning rate
    'batch_size': (16, 128)        # Batch size
}

# 初始化贝叶斯优化器
optimizer = BayesianOptimization(
    f=train_lstm_model,
    pbounds=pbounds,
    random_state=42
)

# 执行贝叶斯优化：初始采样5次，迭代10次（可根据需要调整）
optimizer.maximize(init_points=5, n_iter=10)

# 输出最佳超参数及结果
print("Best parameters from Bayesian Optimization:")
print(optimizer.max)

# 提取最佳超参数
best_params = optimizer.max['params']
best_units = int(best_params['units'])
best_learning_rate = best_params['learning_rate']
best_batch_size = int(best_params['batch_size'])

###############################################################
# Step 4: 构建并训练最终模型（使用最佳超参数）
###############################################################
# 定义构建最终模型的函数（与训练函数类似）

def build_lstm_model(input_shape, num_layers, units, learning_rate):
    """
    Builds and compiles an LSTM model with a variable number of stacked LSTM layers.
    The model starts with an explicit Input layer to define the input shape,
    which avoids passing the input_shape directly to a layer and suppresses related warnings.
    
    Parameters:
      - input_shape (tuple): The shape of the input data (time_steps, features), e.g., (80, 1).
      - num_layers (int): Number of LSTM layers to be stacked.
      - units (int): Number of units for each LSTM layer.
      - learning_rate (float): Learning rate for the Adam optimizer.
      
    Returns:
      - model: A compiled Keras Sequential model.
    """
    model = Sequential()
    # Explicitly define the input shape using an Input layer.
    model.add(Input(shape=input_shape))
    
    # Dynamically add LSTM layers based on the num_layers parameter.
    if num_layers == 1:
        # For a single LSTM layer, no need to return sequences.
        model.add(LSTM(units, return_sequences=False))
    else:
        # For the first LSTM layer, output the full sequence for further processing.
        model.add(LSTM(units, return_sequences=True))
        # Add intermediate LSTM layers if num_layers > 2.
        for _ in range(num_layers - 2):
            model.add(LSTM(units, return_sequences=True))
        # Add the final LSTM layer, which returns only the last output.
        model.add(LSTM(units, return_sequences=False))
    
    # Add a Dense layer for the regression output.
    model.add(Dense(1))
    
    # Compile the model with the Adam optimizer and mean squared error loss.
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')
    
    return model



# 构建最终模型
final_model = build_lstm_model((X_train.shape[1], X_train.shape[2]), best_units, best_learning_rate)

# 对最终模型进行较长时间训练（例如20个Epoch）
final_model.fit(X_train, y_train, epochs=20, batch_size=best_batch_size, verbose=1)

###############################################################
# Step 5: 模型评估与结果展示
###############################################################
# 在测试集上进行预测
predictions = final_model.predict(X_test)

# 计算均方误差（MSE）并求平方根得到 RMSE
mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
print(f"Test RMSE: {rmse}")

# 绘制预测结果与实际值对比图
plt.figure(figsize=(12, 6))
plt.plot(y_test, label="Actual Values")
plt.plot(predictions, label="Predicted Values", linestyle="--")
plt.title("Actual vs Predicted Values")
plt.xlabel("Sample Index")
plt.ylabel("Value")
plt.legend()
plt.grid()
plt.show()

